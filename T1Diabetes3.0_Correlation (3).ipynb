{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "    var toggleCode= function() {\n",
       "        var ins = document.getElementsByClassName(\"CodeMirror\");\n",
       "        for (var i=0; i < ins.length; i++) {\n",
       "            ins[i].style.display= ins[i].style.display === \"none\" ? \"block\": \"none\";\n",
       "        }\n",
       "    };\n",
       "</script>\n",
       "<button onclick=toggleCode()>Toggle Code</button>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<script>\n",
    "    var toggleCode= function() {\n",
    "        var ins = document.getElementsByClassName(\"CodeMirror\");\n",
    "        for (var i=0; i < ins.length; i++) {\n",
    "            ins[i].style.display= ins[i].style.display === \"none\" ? \"block\": \"none\";\n",
    "        }\n",
    "    };\n",
    "</script>\n",
    "<button onclick=toggleCode()>Toggle Code</button>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type 1 Diabetes Correlation Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "------------\n",
    "\n",
    "In this scenario we use AURIN's openapi to access dataset and try to find correlation factors about Type 1 Diabetes in Melbourne.\n",
    "\n",
    ">**Data:**\n",
    "\n",
    "> - Datasets available online from AURIN's openapi.\n",
    "\n",
    "> - 'shapefiles' and 'excel' file about Type 1 Diabetes patient number and the SA2/postcode aggregation type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collecting\n",
    "--------\n",
    "After applying for the authentation, sending request to access AURIN's openapi and searching datasets based on their subjects.\n",
    "\n",
    "For Type 1 Diabetes(T1D) data, reading data from source file.\n",
    "### Extract data from OpenAPI\n",
    "To check all available datasets in AURIN, please visit: https://aurin.org.au/datasets-available-in-the-openapi/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to access with url...\n",
      "------------------------------------------------------------------------\n",
      "Get capabilities successfully.\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Requesting to get the capabilities of the metadata service in AURIN.\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import configparser\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "from collections import defaultdict\n",
    "from dbfread import DBF\n",
    "from lxml import etree\n",
    "\n",
    "# Import authentication parameters from the config file\n",
    "config = configparser.RawConfigParser()\n",
    "config.read('openapi.cfg')\n",
    "\n",
    "#username: student\n",
    "#password: dj78dfGF\n",
    "\n",
    "username=config.get('Auth', 'username')\n",
    "password=config.get('Auth', 'password')\n",
    "\n",
    "# Submit an authenticated request to the AURIN Open API\n",
    "def openapi_request(url):\n",
    "\n",
    "    # create an authenticated HTTP handler and submit URL\n",
    "    password_manager = urllib.request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    password_manager.add_password(None, url, username, password)\n",
    "    auth_manager = urllib.request.HTTPBasicAuthHandler(password_manager)\n",
    "    opener = urllib.request.build_opener(auth_manager)\n",
    "    urllib.request.install_opener(opener)\n",
    "    \n",
    "    #user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'\n",
    "    #headers = { 'User-Agent' : user_agent }\n",
    "    req = urllib.request.Request(url)\n",
    "    try:\n",
    "        handler = urllib.request.urlopen(req)\n",
    "        return handler.read()\n",
    "    except urllib.error.HTTPError as err:\n",
    "        if err.code == 404:\n",
    "            #When requesting the metadata by url, the server limits the successful times, so when\n",
    "            #meeting HTTPError 404, print the message and request again.\n",
    "            print ('Trying to access with url...')\n",
    "            return openapi_request(url)\n",
    "        else:\n",
    "            raise\n",
    "        \n",
    "        \n",
    "# Get the capabilities of the metadata service\n",
    "url = 'http://openapi.aurin.org.au/csw?request=GetCapabilities&service=CSW'\n",
    "xml = openapi_request(url)\n",
    "root = etree.fromstring(xml)\n",
    "print (\"------------------------------------------------------------------------\")\n",
    "print (\"Get capabilities successfully.\")\n",
    "print (\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to access with url...\n",
      "Please choose subject you are interested in from:\n",
      "{'crime', 'Queensland', 'born overseas', 'occupation', 'centroid', 'farms', 'salary', 'irsad', 'picnic', 'urban design', 'personal income', 'school type', 'broadhectare', 'facility', 'floor space ratio', 'education', 'unpaid work', 'New South Wales', 'households', '2015', 'census', 'HOB', 'polygon', 'height of building', 'equine', 'future', 'person characteristics', 'state heritage register', 'advantage', 'bus', 'housing', 'walkability', 'Victoria', 'qualifications', 'deaths', 'LSZ', 'total dwelling count', 'point', 'vicmap', 'public', 'ownership', 'LAP', 'queensland', 'road', 'diversity', 'health', 'school location', 'school', 'ptv', 'curtilage', 'South Australia', 'internet', 'residential', 'School', 'land use', 'expenditure', 'heritage', 'allocation', 'parcel', 'gender', 'child', 'profiles', 'ier', 'Brisbane', 'national', 'cadastre', 'industry', 'businesses', 'human', 'population', 'statistical', 'business exits', 'boundaries', 'play', '2014', 'urban development program', 'LZN', 'wage', 'dog', '2017', 'walkable', 'seifa', 'regional profile', 'sydney', '2016', 'building approvals', 'instrument', 'registered motor vehicle', 'protected areas', 'property', 'route', 'labour force', 'internal migration', 'NSW', 'development', 'averages', 'death', 'land', 'Department of Education and Training', 'Adelaide', 'sport', 'business entries', 'human environment', 'non-government', 'camping', 'beach', 'lga', 'industrial', 'ABS death registration', 'births', 'disadvantage', 'street', 'planning', 'energy', 'ieo', 'government', 'language', 'mortality', 'location', 'areas', 'land reservation acquisition', 'FSR', 'people', 'basketball', 'liquor', 'transport', 'victoria', 'argriculture', 'open space', 'licence', 'social engagement', 'irsd', 'real estate', 'dwelling', 'skate', 'growth', 'agricultural land', 'nsw', 'projections', 'statistics', 'recreation', 'nrp', 'gambling', 'mesh blocks', 'HER', 'youth engagement', 'village', 'family', 'license', 'families', 'medians', 'agricultural commodities', 'local government area', 'gaming', 'urban planning', 'society', 'economy', 'minimum subdivision lot size', 'water', 'new south wales', 'LRA', '2013', 'vif', 'service', 'Australian Bureau of Statistics', 'park', 'employed', 'budget', '2012', 'biophysical', 'erp', 'agricultural production', 'total usual resident population', 'environment', 'rent', 'electricity consumption', 'proposed', 'broad hectare', 'heritage conservation', 'tourist accommodation establishments', 'Australian Capital Territory', 'facilities', 'Canberra', 'ACT', 'mortgage', 'viticulture', 'udp', 'estimated resident population', 'rural', 'travel to work', 'socio-economic', 'land application'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ DATASET + SUBJECTS ================\n",
      "aurin:datasource-AU_Govt_ABS-UoM_AURIN_DB_1_nrp_econ_aust_sa2\n",
      "census\n",
      "regional profile\n",
      "nrp\n",
      "economy\n",
      "businesses\n",
      "business entries\n",
      "business exits\n",
      "industry\n",
      "labour force\n",
      "youth engagement\n",
      "personal income\n",
      "wage\n",
      "salary\n",
      "occupation\n",
      "rent\n",
      "mortgage\n",
      "building approvals\n"
     ]
    }
   ],
   "source": [
    "def get_all_datasets():\n",
    "    dataset_names = []#This is a list of dataset names we can use to access its features\n",
    "\n",
    "    # Get a list of all available datasets and their licences\n",
    "    url='http://openapi.aurin.org.au/csw?request=GetRecords&service=CSW&version=2.0.2&typeNames=csw:Record&elementSetName=full&resultType=results&constraintLanguage=CQL_TEXT&constraint_language_version=1.1.0&maxRecords=1000'\n",
    "    xml = openapi_request(url)\n",
    "    root = etree.fromstring(xml)\n",
    "     \n",
    "    subs = set()#save all subjects available to choose\n",
    "    for dataset in root.findall(\".//csw:Record\", root.nsmap):\n",
    "        for subject in dataset.findall(\".//dc:subject\",root.nsmap):\n",
    "            if not subject.text.startswith('ANZ'):\n",
    "                subs.add(subject.text)\n",
    "    print ('Please choose subject you are interested in from:')\n",
    "    print (subs)\n",
    "    target_sub = input('I want to search:')\n",
    "     \n",
    "    for dataset in root.findall(\".//csw:Record\", root.nsmap):\n",
    "        #Start searching relavant datasets\n",
    "        flag = 0\n",
    "        for subject in dataset.findall(\".//dc:subject\",root.nsmap):\n",
    "            if subject.text == target_sub:\n",
    "                flag = 1\n",
    "                break\n",
    "        #Printing\n",
    "        if flag == 1:\n",
    "            print('================ DATASET + SUBJECTS ================')\n",
    "            name = dataset.find(\".//dc:identifier\",root.nsmap).text\n",
    "            dataset_names.append(name)\n",
    "            print (name)\n",
    "            for subject in dataset.findall(\".//dc:subject\",root.nsmap):\n",
    "                print (subject.text) \n",
    "                \n",
    "    return dataset_names\n",
    "#get all available dataset names\n",
    "dataset_names = get_all_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found datasets below: ['aurin:datasource-AU_Govt_ABS-UoM_AURIN_DB_1_nrp_econ_aust_sa2']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have selected: aurin:datasource-AU_Govt_ABS-UoM_AURIN_DB_1_nrp_econ_aust_sa2\n"
     ]
    }
   ],
   "source": [
    "#Let user choose dataset.\n",
    "if dataset_names == []:\n",
    "    raise Exception('No valid dataset is selected.')\n",
    "else:\n",
    "    print ('We found datasets below:',dataset_names)\n",
    "    i = input('Please select the index that you want:')\n",
    "    name = dataset_names[int(i)]\n",
    "    print ('You have selected:',name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to access with url...\n",
      "Property_name is:gid\n",
      "Property_name is:sa2_main11\n",
      "Property_name is:sa2_name11\n",
      "Property_name is:year_\n",
      "Property_name is:cabee_2\n",
      "Property_name is:cabee_3\n",
      "Property_name is:cabee_4\n",
      "Property_name is:cabee_5\n",
      "Property_name is:cabee_7\n",
      "Property_name is:cabee_8\n",
      "Property_name is:cabee_9\n",
      "Property_name is:cabee_10\n",
      "Property_name is:cabee_12\n",
      "Property_name is:cabee_13\n",
      "Property_name is:cabee_14\n",
      "Property_name is:cabee_15\n",
      "Property_name is:cabee_17\n",
      "Property_name is:cabee_18\n",
      "Property_name is:cabee_19\n",
      "Property_name is:cabee_20\n",
      "Property_name is:cabee_21\n",
      "Property_name is:cabee_22\n",
      "Property_name is:cabee_23\n",
      "Property_name is:cabee_24\n",
      "Property_name is:cabee_25\n",
      "Property_name is:cabee_26\n",
      "Property_name is:cabee_27\n",
      "Property_name is:cabee_28\n",
      "Property_name is:cabee_29\n",
      "Property_name is:cabee_30\n",
      "Property_name is:cabee_31\n",
      "Property_name is:cabee_32\n",
      "Property_name is:cabee_33\n",
      "Property_name is:cabee_34\n",
      "Property_name is:cabee_35\n",
      "Property_name is:cabee_36\n",
      "Property_name is:cabee_37\n",
      "Property_name is:lf_2\n",
      "Property_name is:lf_3\n",
      "Property_name is:lf_4\n",
      "Property_name is:lf_5\n",
      "Property_name is:youth_2\n",
      "Property_name is:youth_3\n",
      "Property_name is:youth_4\n",
      "Property_name is:youth_5\n",
      "Property_name is:youth_6\n",
      "Property_name is:youth_7\n",
      "Property_name is:youth_8\n",
      "Property_name is:youth_9\n",
      "Property_name is:income_2\n",
      "Property_name is:income_3\n",
      "Property_name is:income_4\n",
      "Property_name is:income_5\n",
      "Property_name is:income_6\n",
      "Property_name is:income_7\n",
      "Property_name is:income_8\n",
      "Property_name is:income_9\n",
      "Property_name is:income_10\n",
      "Property_name is:income_11\n",
      "Property_name is:income_12\n",
      "Property_name is:income_13\n",
      "Property_name is:income_14\n",
      "Property_name is:income_15\n",
      "Property_name is:income_16\n",
      "Property_name is:income_17\n",
      "Property_name is:income_18\n",
      "Property_name is:income_19\n",
      "Property_name is:wage_2\n",
      "Property_name is:wage_3\n",
      "Property_name is:wage_6\n",
      "Property_name is:wage_7\n",
      "Property_name is:wage_8\n",
      "Property_name is:wage_9\n",
      "Property_name is:wage_10\n",
      "Property_name is:wage_11\n",
      "Property_name is:wage_12\n",
      "Property_name is:wage_14\n",
      "Property_name is:wage_15\n",
      "Property_name is:wage_16\n",
      "Property_name is:wage_17\n",
      "Property_name is:wage_18\n",
      "Property_name is:wage_19\n",
      "Property_name is:wage_20\n",
      "Property_name is:wage_22\n",
      "Property_name is:wage_23\n",
      "Property_name is:wage_24\n",
      "Property_name is:wage_25\n",
      "Property_name is:wage_26\n",
      "Property_name is:wage_27\n",
      "Property_name is:wage_28\n",
      "Property_name is:wage_30\n",
      "Property_name is:wage_31\n",
      "Property_name is:wage_32\n",
      "Property_name is:wage_33\n",
      "Property_name is:wage_34\n",
      "Property_name is:wage_35\n",
      "Property_name is:wageocc_2\n",
      "Property_name is:wageocc_3\n",
      "Property_name is:wageocc_4\n",
      "Property_name is:wageocc_5\n",
      "Property_name is:wageocc_6\n",
      "Property_name is:wageocc_7\n",
      "Property_name is:wageocc_8\n",
      "Property_name is:wageocc_9\n",
      "Property_name is:wageocc_10\n",
      "Property_name is:wageocc_11\n",
      "Property_name is:wageocc_12\n",
      "Property_name is:wageocc_13\n",
      "Property_name is:wageocc_14\n",
      "Property_name is:wageocc_15\n",
      "Property_name is:wageocc_16\n",
      "Property_name is:wageocc_17\n",
      "Property_name is:wageocc_18\n",
      "Property_name is:wageocc_19\n",
      "Property_name is:wageocc_20\n",
      "Property_name is:rent_2\n",
      "Property_name is:rent_3\n",
      "Property_name is:bldg_2\n",
      "Property_name is:bldg_3\n",
      "Property_name is:bldg_4\n",
      "Property_name is:bldg_5\n",
      "Property_name is:bldg_6\n",
      "Property_name is:bldg_7\n",
      "Property_name is:bldg_8\n",
      "Property_name is:bldg_9\n",
      "Property_name is:bldg_10\n",
      "Property_name is:bldg_11\n",
      "Property_name is:shape_leng\n",
      "Property_name is:shape_area\n",
      "Property_name is:geom\n",
      "['sa2_main11']\n"
     ]
    }
   ],
   "source": [
    "# Query the attributes of the selected dataset\n",
    "def getFeatures(dataset):\n",
    "    url = 'http://openapi.aurin.org.au/wfs?request=DescribeFeatureType&service=WFS&version=1.1.0&typeName='+name\n",
    "    \n",
    "    xml = openapi_request(url)\n",
    "    root = etree.fromstring(xml)\n",
    "    property_name = ''\n",
    "    property_dict = defaultdict(list)\n",
    "    agglevel = '' #either sa2 or postcode\n",
    "    keylist = [] #candidate key names\n",
    "    for element in root.find(\".//xsd:sequence\", root.nsmap):\n",
    "        property_name = element.get('name')\n",
    "        property_dict[property_name] = None\n",
    "        print ('Property_name is:'+property_name)\n",
    "        if 'post' in property_name and 'code' in property_name:\n",
    "            agglevel = 'postcode'\n",
    "            keylist.append(property_name)\n",
    "        elif 'sa2' in property_name and 'main' in property_name:\n",
    "            agglevel = 'sa2'\n",
    "            keylist.append(property_name)\n",
    "    if keylist == []:\n",
    "        raise Exception('Invalid dataset. No valid aggregation level feature is included.')\n",
    "    print (keylist)\n",
    "    keyname = input('Please input a valid keyname:')\n",
    "    if agglevel == '':\n",
    "        raise Exception('Error input!')\n",
    "    return property_dict,keyname,agglevel\n",
    "\n",
    "#property_dict: save all features' name as keys and the value of some features will be saved\n",
    "#keyname: It is the special feature which records the aggregation level. e.g. 'sa2_main11','actual_postcode' \n",
    "#agg_level: 'sa2' or 'postcode'\n",
    "property_dict,keyname,agg_level = getFeatures(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the decimal longitude and latitude to Web Mercator Format, make it easier for data visualization\n",
    "import math\n",
    "def toWebMercator(xLon, yLat):\n",
    "    # Check if coordinate out of range for Latitude/Longitude\n",
    "    if (abs(xLon) > 180) and (abs(yLat) > 90):\n",
    "        return\n",
    " \n",
    "    semimajorAxis = 6378137.0  # WGS84 spheriod semimajor axis\n",
    "    east = xLon * 0.017453292519943295\n",
    "    north = yLat * 0.017453292519943295\n",
    " \n",
    "    northing = 3189068.5 * math.log((1.0 + math.sin(north)) / (1.0 - math.sin(north)))\n",
    "    easting = semimajorAxis * east\n",
    " \n",
    "    return [easting, northing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define geometry data collecting methods. \n",
    "There are two ways to get geometry data. \n",
    "1. Getting coordinates for polygons from shapefile(ENDIA file) when agg_level is SA2\n",
    "2. Collecting coordinates data directly from url when agg_level is postcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Geometry data from the shapefile about t2d patient.\n",
    "import shapefile\n",
    "from geojson import Polygon\n",
    "%matplotlib inline\n",
    "def getGeometry_sa2(filename):\n",
    "    poly_dict = defaultdict(Polygon)#key: sa2_main11 value:Polygon\n",
    "    \n",
    "    sf = shapefile.Reader(filename)\n",
    "    #print (sf.fields)\n",
    "    #395 in total\n",
    "    for shape in sf.shapeRecords():\n",
    "        poly = []\n",
    "        key = shape.record[1]\n",
    "        #print (key)\n",
    "        for longi,lati in shape.shape.points:\n",
    "            webMer_xy = toWebMercator(longi, lati)\n",
    "            poly.append(webMer_xy)\n",
    "            \n",
    "        poly = Polygon([poly])\n",
    "        poly_dict[str(key)]=poly    \n",
    "    return poly_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Geometry data from the shapefile about t2d patient.\n",
    "import shapefile\n",
    "from geojson import Polygon,Point\n",
    "%matplotlib inline\n",
    "def getGeometry_postcode(dataset_name,keyname):\n",
    "    #Plotting shape. 0 means drawing Points; 1 means drawing Polygon\n",
    "    geo_shape = -1\n",
    "    #Prepare to print progress\n",
    "    barLength = 40\n",
    "    \n",
    "    #This can get all data with all features at the same time\n",
    "    url = 'http://openapi.aurin.org.au/wfs?request=GetFeature&service=WFS&version=2.0.0&TypeName='+dataset_name\n",
    "    \n",
    "    #print('================ VALUES FOR A FEATURE ================')\n",
    "    print('Query URL: '+url)\n",
    "    \n",
    "    xml = openapi_request(url)\n",
    "    root = etree.fromstring(xml)\n",
    "    instance_plots = []\n",
    "    count = 0 \n",
    "    poly_list = []#has a list of polygon\n",
    "    #poly_dict = defaultdict(list)\n",
    "    poly_dict = defaultdict(Polygon)\n",
    "    total_instance = len(root.findall('.//wfs:member',root.nsmap))#total number of instances\n",
    "    print ('Number of instance is',total_instance)\n",
    "    \n",
    "    for member in root.findall('.//wfs:member',root.nsmap):\n",
    "        count += 1\n",
    "        #each instance's geometry data are saved in a list\n",
    "        ins_geometry = []\n",
    "        \n",
    "        #Get key value by searching element tag.\n",
    "        key = ''\n",
    "        for keyitem in member.iterfind('.//aurin:'+keyname,root.nsmap):\n",
    "            key = keyitem.text\n",
    "        \n",
    "        #Find Polygon information 'posList'\n",
    "        for poslist in member.iterfind('.//gml:posList',root.nsmap):\n",
    "            geo_shape = 1 #drawing polygons later\n",
    "            \n",
    "            poslist = [float(x) for x in poslist.text.split()]\n",
    "            #limit the range of longitude value\n",
    "            lati,longi=[],[]\n",
    "            poly = []\n",
    "            for index in range(0,len(poslist)-1,2):\n",
    "                lati.append(poslist[index])\n",
    "                longi.append(poslist[index+1])\n",
    "                \n",
    "                #Since we plot polygons based on map which requires Web Mercator format\n",
    "                webMer_xy = toWebMercator(poslist[index+1], poslist[index])\n",
    "                poly.append(webMer_xy)\n",
    "                \n",
    "            ins_geometry.append((lati,longi))\n",
    "            \n",
    "            poly = Polygon([poly])\n",
    "            \n",
    "            poly_dict[key] = poly\n",
    "            \n",
    "            break #only go into the inner loop for once\n",
    "        \n",
    "        #Find Point information 'pos'\n",
    "        for pos in member.iterfind('.//gml:pos',root.nsmap):\n",
    "            geo_shape = 0 # Drawing Points later\n",
    "            \n",
    "            pos = [float(x) for x in pos.text.split()]\n",
    "            \n",
    "            #limit the range of longitude value\n",
    "            lati,longi=pos[0],pos[1]\n",
    "            \n",
    "            #Since we plot polygons/points based on map which requires Web Mercator format\n",
    "            webMer_xy = toWebMercator(longi, lati)\n",
    "            \n",
    "            poly = Point(webMer_xy)\n",
    "            poly_dict[key] = poly\n",
    "            \n",
    "            break #only go into the inner loop for once\n",
    "        #Print progress bar\n",
    "        if count%100==0:\n",
    "            progress = float(count)/total_instance\n",
    "            block = int(round(barLength*progress))\n",
    "            text = \"\\rPercent: [{0}] {1}%\".format( \"=\"*block + \" \"*(barLength-block), progress*100)\n",
    "            print (text) \n",
    "    #Print the progress bar with 100% \n",
    "    block = barLength\n",
    "    text = \"\\rPercent: [{0}] {1}%\".format( \"=\"*block + \" \"*(barLength-block), 100)\n",
    "    print (text)  \n",
    "    print (count,len(poly_dict))\n",
    "    return poly_dict,geo_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify features we are interested in. \n",
    "To make the tool  more efficient and durable, we limit the number of selected feature within 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features we have now are: dict_keys(['bldg_11', 'cabee_26', 'bldg_6', 'wageocc_17', 'bldg_3', 'year_', 'wageocc_10', 'wage_34', 'income_18', 'sa2_main11', 'wageocc_13', 'income_11', 'wage_35', 'cabee_12', 'wage_25', 'bldg_2', 'wageocc_7', 'cabee_35', 'wage_8', 'income_5', 'income_10', 'income_13', 'wage_6', 'cabee_36', 'cabee_25', 'wage_26', 'wageocc_4', 'wageocc_20', 'income_8', 'wage_15', 'lf_5', 'wage_9', 'cabee_8', 'youth_6', 'cabee_32', 'wage_33', 'income_4', 'cabee_2', 'wage_22', 'cabee_27', 'wage_7', 'wage_2', 'cabee_18', 'youth_8', 'shape_area', 'youth_3', 'cabee_15', 'cabee_10', 'youth_2', 'wageocc_18', 'bldg_5', 'income_6', 'wageocc_15', 'bldg_9', 'youth_9', 'wageocc_12', 'cabee_19', 'wage_11', 'wageocc_8', 'income_19', 'wage_32', 'wageocc_3', 'wage_19', 'wageocc_6', 'wage_23', 'cabee_22', 'cabee_34', 'sa2_name11', 'income_12', 'rent_2', 'cabee_20', 'cabee_24', 'wage_20', 'wage_18', 'bldg_10', 'bldg_8', 'wage_31', 'cabee_29', 'income_16', 'lf_4', 'wage_27', 'cabee_21', 'bldg_4', 'wage_3', 'wageocc_16', 'cabee_17', 'cabee_37', 'income_14', 'cabee_14', 'cabee_13', 'rent_3', 'youth_7', 'income_15', 'cabee_28', 'cabee_23', 'cabee_5', 'wage_12', 'wageocc_14', 'income_3', 'wageocc_11', 'gid', 'cabee_33', 'youth_5', 'income_7', 'income_17', 'lf_2', 'wageocc_5', 'cabee_31', 'wage_10', 'bldg_7', 'income_2', 'wageocc_2', 'wageocc_9', 'lf_3', 'wage_28', 'wageocc_19', 'wage_17', 'shape_leng', 'cabee_30', 'cabee_7', 'geom', 'wage_30', 'income_9', 'wage_16', 'wage_14', 'cabee_3', 'cabee_4', 'wage_24', 'cabee_9', 'youth_4'])\n",
      "Selected features are: {'cabee_12', 'income_5', 'sa2_main11'}\n"
     ]
    }
   ],
   "source": [
    "prop_list = property_dict.keys()\n",
    "max_feature_num = 3\n",
    "#let users select features they want to analysis\n",
    "#Asumming the feature selected mush be integer or float.\n",
    "def selectFeatures():\n",
    "    \n",
    "    print ('Total features we have now are:',prop_list)\n",
    "    \n",
    "    selected = input(\"I select:\") \n",
    "    #sa2_main11 or postcode can be seen as the key item in choose, so user does not need to add it. \n",
    "    choose = set([keyname])\n",
    "    for name in selected.split():\n",
    "        if name in prop_list:\n",
    "            choose.add(name)\n",
    "            if len(choose)==max_feature_num:\n",
    "                break       \n",
    "    return choose\n",
    "\n",
    "#choose: saving all selected features and the keyname(the feature which specifies aggregation level)\n",
    "choose = selectFeatures()\n",
    "print ('Selected features are:',choose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a feature, getFeatureValue will return all values.\n",
    "def getFeatureValue(prop):\n",
    "    #This can get all values for property_name\n",
    "    url = 'http://openapi.aurin.org.au/wfs?request=GetPropertyValue&service=WFS&version=2.0.0&TypeName='+name+'&valueReference='+prop\n",
    "    #print (url)\n",
    "    \n",
    "    xml = openapi_request(url)\n",
    "    root = etree.fromstring(xml)\n",
    "    #i = 0\n",
    "    values = []\n",
    "    for member in root.findall('.//wfs:member',root.nsmap):\n",
    "        value = member.find('.//aurin:'+prop,root.nsmap).text\n",
    "        values.append(value) \n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting geometry data. \n",
    "After selecting features needed to fetch values, we start collecting geometry data and the type.(Points or Polygons) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Geometry data successfully collected\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "geo_shape = -1\n",
    "if agg_level == 'sa2':\n",
    "    #key is sa2 code, value is Polygon objective.\n",
    "    polygon_dict = getGeometry_sa2('endia-sa2/04eba726-cea8-4ba6-8911-0be82949c851.shp')\n",
    "    geo_shape = 1 #Drawing polygons later\n",
    "elif agg_level == 'postcode':\n",
    "    polygon_dict,geo_shape = getGeometry_postcode(name,keyname)\n",
    "    \n",
    "print (\"------------------------------------------------------------------------\")\n",
    "print (\"Geometry data successfully collected\")\n",
    "print (\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting normal featuers' values.\n",
    "*when seeing message 'HTTPError 404', please ignore because it will not affect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to access with url...\n",
      "Total instance number is 11021 and finish collecting property \"sa2_main11\"\n",
      "Trying to access with url...\n",
      "Total instance number is 11021 and finish collecting property \"cabee_12\"\n",
      "Trying to access with url...\n",
      "Total instance number is 11021 and finish collecting property \"income_5\"\n",
      "------------------------------------------------------------------------\n",
      "Completed collected normal features values.\n",
      "Updated candidate features are ['sa2_main11', 'cabee_12', 'income_5']\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#collect values for keyname\n",
    "property_dict[keyname] = getFeatureValue(keyname)\n",
    "instance_num = len(property_dict[keyname])\n",
    "total_ins = instance_num\n",
    "print ('Total instance number is %d and finish collecting property \\\"%s\\\"' %(instance_num,keyname))\n",
    "\n",
    "#If a features values is not complete (smaller than total_ins), discard it.\n",
    "new_choose = [keyname]\n",
    "#collect values for normal features\n",
    "for prop in prop_list:\n",
    "    if prop in choose and prop != keyname:\n",
    "        property_dict[prop] = getFeatureValue(prop)\n",
    "        instance_num = len(property_dict[prop])\n",
    "        if instance_num == total_ins:\n",
    "            new_choose.append(prop)\n",
    "            print ('Total instance number is %d and finish collecting property \\\"%s\\\"' %(instance_num,prop))\n",
    "        else:\n",
    "            print ('Property',prop,'has insufficient values, will remove it from choose')\n",
    "choose = new_choose\n",
    "print (\"------------------------------------------------------------------------\")\n",
    "print ('Completed collected normal features values.')\n",
    "print ('Updated candidate features are',choose)\n",
    "print (\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read T1D data from source data\n",
    "-Read shapefile from ENDIA when aggregation level is SA2\n",
    "\n",
    "-Read csv file from ADDN when aggregation level is postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Aggregation level code successfully collected.\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#patient_dict is a dictionary which keeps keyname as key and patient_num as value.\n",
    "import csv\n",
    "patient_dict = defaultdict(int)\n",
    "\n",
    "#Collect patient distribution at SA2 level.\n",
    "if agg_level == 'sa2':\n",
    "    for record in DBF('endia-sa2/04eba726-cea8-4ba6-8911-0be82949c851.dbf'):\n",
    "        patient_dict[str(record['SA2_MAIN11'])] = int(record['count'])\n",
    "    #print ('The number of different aggregation level areas is:',len(patient_dict.keys()))\n",
    "#Collect patient distribution at postcode level.   \n",
    "elif agg_level == 'postcode':\n",
    "    print ('Please choose 0(diagnosed postcode); 1(current postcode)')\n",
    "    index = input('I select')\n",
    "    index = int(index)\n",
    "    if index != 0 and index != 1:\n",
    "        raise Exception('Input error!')\n",
    "    with open('postcode_pairs.csv', 'r') as infile:\n",
    "        r = csv.reader(infile)\n",
    "        next(r)\n",
    "        for row in r: \n",
    "            patient_dict[str(row[index])] = int(row[2])\n",
    "    #print ('The number of different aggregation level areas is:',len(patient_dict.keys()))\n",
    "print (\"------------------------------------------------------------------------\")\n",
    "print ('Aggregation level code successfully collected.')\n",
    "print (\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comebine all data together. \n",
    "normal features' values<-->geometry data<-->patients' information\n",
    "\n",
    "Using aggregation level(sa2/postcode) as bridges to save all relative information together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dic:(sa2_code:dictionary). With all normal features' values.\n",
    "def combineDict(dic,patient_dict,geom_dict):\n",
    "    #Each item in instances is a dict with all information.\n",
    "    #If common keyname's value occure, put all informatin together as one instance.\n",
    "    instances = []\n",
    "    for k,list_val in dic.items():\n",
    "        if len(list_val) == 1:\n",
    "            list_val[0].update({'geometry':geom_dict[k],'count':int(patient_dict[k])})\n",
    "            instances.append(list_val[0])\n",
    "        elif len(list_val)>1:\n",
    "            basedict = list_val[0]\n",
    "            basedict.update({'count':int(patient_dict[k])})\n",
    "            basedict.update({'geometry':geom_dict[k]})\n",
    "            \n",
    "            for newdict in list_val[1:]:\n",
    "                for prop in choose:\n",
    "                    if 'geom' not in prop and prop != keyname:\n",
    "                        #if values are countable, add them up, otherwise, ues commans to divide.\n",
    "                        try:\n",
    "                            basedict[prop] = float(basedict[prop])\n",
    "                            newdict[prop] = float(newdict[prop])\n",
    "                            basedict[prop] += newdict[prop]\n",
    "                        except ValueError:\n",
    "                            basedict[prop] += ','+newdict[prop]\n",
    "            instances.append(basedict)\n",
    "        else:\n",
    "            \"error!\"\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Format change completed.\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from geojson import Feature,FeatureCollection,Polygon,GeometryCollection\n",
    "import geojson\n",
    "# property_dict save property_name as key, and a dictionary containing type and value as value.\n",
    "# Now we need to change the format by saving a list of Instance objects which represent many instances(row).\n",
    "def changeFormat(property_dict,candidates):\n",
    "    instance_dict = defaultdict(list) #key=sa2_main11 value:dict\n",
    "    for i in range(instance_num):\n",
    "        #check whether the instance occur in the t2d file.\n",
    "        if property_dict[keyname][i] not in patient_dict.keys():\n",
    "            continue\n",
    "        #print (property_dict['sa2_main11']['value'][i])\n",
    "        #properties = dict() #save all values except geometry\n",
    "        ins = dict()\n",
    "        for k,v in property_dict.items():\n",
    "            if k in candidates:\n",
    "                if k == keyname:\n",
    "                    #update attribute in instance\n",
    "                    ins.update({k:str(v[i])})\n",
    "                elif 'geom' not in k:\n",
    "                    #update attribute in instance\n",
    "                    ins.update({k:str(v[i])})\n",
    "                    \n",
    "        instance_dict[ins[keyname]].append(ins)\n",
    "        \n",
    "    instances = combineDict(instance_dict,patient_dict,polygon_dict) #except combing item with same sa2, but also add count of patient\n",
    "    \n",
    "    #add valid data into feature_list\n",
    "    feature_list = []#for creating a featureCollection to plot later\n",
    "    for instance in instances:\n",
    "        properties = dict()\n",
    "        for k,v in instance.items():\n",
    "            if k != 'geometry':\n",
    "                properties.update({k:v})\n",
    "        #print (type(instance),type(instance['geometry']))\n",
    "        feature = Feature(geometry=instance['geometry'], properties=properties)\n",
    "        feature_list.append(feature)\n",
    "        \n",
    "    return instances,feature_list\n",
    "\n",
    "\n",
    "instances,feature_list = changeFormat(property_dict,choose)  \n",
    "print (\"------------------------------------------------------------------------\")\n",
    "print ('Format change completed.')\n",
    "print (\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "--------\n",
    "Based on the number of T1D patients in specific SA2 area, map the data to a Australia map and using different colors to represent differencet number of patients.\n",
    "\n",
    "If the dataset is at postcode level, we draw blue points to represent different postcode areas. In this case, number of T1D patients cannot be reflected by color but can be seen in the HoverTool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geojson import FeatureCollection\n",
    "\n",
    "fc = FeatureCollection(feature_list)\n",
    "geojsonData = geojson.dumps(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"4e34150e-fbca-4a21-a58c-8de5ea8e7a3c\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = \"1\";\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      Bokeh.$(\"#4e34150e-fbca-4a21-a58c-8de5ea8e7a3c\").text(\"BokehJS successfully loaded.\");\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"4e34150e-fbca-4a21-a58c-8de5ea8e7a3c\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '4e34150e-fbca-4a21-a58c-8de5ea8e7a3c' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      Bokeh.$(\"#4e34150e-fbca-4a21-a58c-8de5ea8e7a3c\").text(\"BokehJS is loading...\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.3.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.3.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === \"1\") {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (!force) {\n",
       "      var cell = $(\"#4e34150e-fbca-4a21-a58c-8de5ea8e7a3c\").parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Base map layer,showing AU map\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.tile_providers import WMTSTileSource\n",
    "\n",
    "AU = x_range,y_range = ((11884029,17283304), (-688291,-4556972))\n",
    "\n",
    "#fig = figure(tools='pan, wheel_zoom', x_range=x_range, y_range=y_range)\n",
    "TOOLS=\"pan,wheel_zoom,box_zoom,hover,save\"\n",
    "fig = figure(title=\"Water and Energy consuming data\", tools=TOOLS,x_range=x_range, y_range=y_range)\n",
    "\n",
    "fig.axis.visible = False\n",
    "url = 'http://a.basemaps.cartocdn.com/light_all/{Z}/{X}/{Y}.png'\n",
    "attribution = \"Map tiles by Carto, under CC BY 3.0. Data by OpenStreetMap, under ODbL\"\n",
    "\n",
    "fig.add_tile(WMTSTileSource(url=url, attribution=attribution))\n",
    "output_notebook()\n",
    "#show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import GeoJSONDataSource, ColumnDataSource, HoverTool, LinearColorMapper, LogColorMapper\n",
    "from bokeh.palettes import Purples6 as palettex\n",
    "\n",
    "geo_source = GeoJSONDataSource(geojson=geojsonData)\n",
    "\n",
    "palettex = palettex[::-1]\n",
    "color_mapper = LogColorMapper(palette=palettex)\n",
    "\n",
    "fig.title.text_font_size='12pt'\n",
    "fig.title.align='center'\n",
    "\n",
    "if geo_shape == 0:\n",
    "    #Drawing circles when meeting Points\n",
    "    fig.circle('x','y',size=4,color = 'blue',source=geo_source)\n",
    "elif geo_shape == 1:\n",
    "    #Drawing polygons when meeting Polygons\n",
    "    fig.patches('xs','ys',fill_alpha=.99, fill_color={'field': 'count', 'transform': color_mapper}\n",
    "              ,line_color=\"#884444\", line_width=0.1, line_alpha=0.8,source=geo_source)\n",
    "\n",
    "hover = fig.select_one(HoverTool)\n",
    "hover.point_policy = \"follow_mouse\"\n",
    "display_prop = [('count','@count')]\n",
    "for prop in choose:\n",
    "    if 'geom' not in prop:\n",
    "        display_prop.append((prop,'@'+prop))\n",
    "hover.tooltips = display_prop\n",
    "\n",
    "#output_notebook()\n",
    "show(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "--------\n",
    "The main idea is to see if there is an association between number of patients and countable features that user selected from the dataset. To process discret data, Maximum Information Coefficient(MIC) is used.\n",
    "\n",
    "If there is, we can do some deeper analysis to find a fine model which describe the relationship better.This tool offer three regression models: Linearregression, logicregression and multinomial naive bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we convert all normal feature values types to string. In this part, for countable numbers,\n",
    "# convert them back to float and using MIC to represent their correlation with patient number\n",
    "from minepy import MINE\n",
    "\n",
    "m = MINE()\n",
    "#x records number of patients\n",
    "x = []\n",
    "for ins in instances:\n",
    "    x.append(ins['count'])\n",
    "\n",
    "#countable feature values\n",
    "total_ins = len(instances)\n",
    "ys = defaultdict(list) # save feature name and their values\n",
    "for feature in choose:\n",
    "    y = []\n",
    "    for instance in instances:\n",
    "        try:\n",
    "            value = float(instance[feature])\n",
    "            y.append(value)\n",
    "        except ValueError:\n",
    "            print (\"%s are uncountable.\"% feature)\n",
    "            break\n",
    "    if len(y) == total_ins:\n",
    "        ys[feature] = y\n",
    "print (\"------------------------------------------------------------------------\")\n",
    "print ('Countable values successfully collected.')\n",
    "print (\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize correlation between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawLine(x,y,featurename):\n",
    "    # create a new plot with default tools, using figure\n",
    "    title = \"Correlation bewteen number of patient and \"+featurename\n",
    "    p = figure(title = title,plot_width=500, plot_height=400)\n",
    "    # add a circle renderer with a size, color, and alpha\n",
    "    #p.line(x, y, line_width=2)\n",
    "    p.circle(x, y, line_color=\"navy\", fill_color=\"red\", size=8)\n",
    "    show(p) # show the results\n",
    "    \n",
    "for k,val in ys.items():  \n",
    "    m.compute_score(x,val)\n",
    "    drawLine(x,val,k)\n",
    "    print('Maximal information coefficient between T1D patient number and %s is %f'% (k,m.mic()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further analysis\n",
    "If there is a close relationship between certain features and number of patients, we use three models to help analyse data. \n",
    "Before training data, DictVectorizer is used for creating a sparse matrix, in this case, more features will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build training data set and apply several models to see the model's score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "\n",
    "print (choose)\n",
    "features = input('I select features:')\n",
    "features = features.split()\n",
    "#print (features)\n",
    "\n",
    "feature_matrix = []#training data\n",
    "data_label = []#training set class values\n",
    "for instance in instances:\n",
    "    dict1 = {}\n",
    "    for feature in features:\n",
    "        dict1.update({feature:instance[feature]})\n",
    "    feature_matrix.append(dict1)\n",
    "    data_label.append(instance['count'])\n",
    "        \n",
    "vectorizer = DictVectorizer()\n",
    "data_dataset = vectorizer.fit_transform(feature_matrix).toarray()\n",
    "#print (data_dataset.shape,len(data_label))\n",
    "\n",
    "#divide dataset into train_dataset and dev_dataset\n",
    "total = len(instances)\n",
    "dev_num = total//3 #dev_dataset counts for 1/3 of total dataset\n",
    "\n",
    "dev_dataset = data_dataset[:dev_num]\n",
    "dev_classification = data_label[:dev_num]\n",
    "\n",
    "train_dataset = data_dataset[dev_num:]\n",
    "train_classification = data_label[dev_num:]\n",
    "\n",
    "# Start traing with--------LinearRegression--------------------\n",
    "try:\n",
    "    ln = LinearRegression()\n",
    "    ln.fit(train_dataset, train_classification)          \n",
    "    acc_ln = ln.score(dev_dataset,dev_classification)    \n",
    "    print ('Accuracy for model LinearRegression is',acc_ln)\n",
    "except ValueError:\n",
    "    print ('Current dataset has invalid values, cannot build Linear Regression model.')\n",
    "\n",
    "\n",
    "# Start traing with--------LogisticRegression--------------------\n",
    "try:\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(train_dataset, train_classification)          \n",
    "    acc_lr = lr.score(dev_dataset,dev_classification)\n",
    "    print ('Accuracy for model LogisticRegression is',acc_lr)\n",
    "except ValueError:\n",
    "    print ('Current dataset has invalid values, cannot build Logistic Regression model.')\n",
    "\n",
    "# Start traing with--------Multinomial Naive Bayes--------------------\n",
    "try:\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(train_dataset, train_classification)  \n",
    "    acc_mnb = mnb.score(dev_dataset,dev_classification)\n",
    "    print ('Accuracy for model MultinomialNB is',acc_mnb)\n",
    "except ValueError:\n",
    "    print ('Current dataset has invalid values, cannot build Multinomial Naive Bayes model.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
